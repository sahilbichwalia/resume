{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10457962,"sourceType":"datasetVersion","datasetId":6474054}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:56.360192Z","iopub.execute_input":"2025-01-13T13:31:56.360536Z","iopub.status.idle":"2025-01-13T13:31:56.746686Z","shell.execute_reply.started":"2025-01-13T13:31:56.360506Z","shell.execute_reply":"2025-01-13T13:31:56.745327Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/job-resume/resume_job_description_fit_trains_matching.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/job-resume/resume_job_description_fit_trains_matching.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:56.748047Z","iopub.execute_input":"2025-01-13T13:31:56.748524Z","iopub.status.idle":"2025-01-13T13:31:57.933618Z","shell.execute_reply.started":"2025-01-13T13:31:56.748478Z","shell.execute_reply":"2025-01-13T13:31:57.932445Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:57.935726Z","iopub.execute_input":"2025-01-13T13:31:57.936128Z","iopub.status.idle":"2025-01-13T13:31:57.955290Z","shell.execute_reply.started":"2025-01-13T13:31:57.936096Z","shell.execute_reply":"2025-01-13T13:31:57.954250Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                         resume_text  \\\n0  SummaryHighly motivated Sales Associate with e...   \n1  Professional SummaryCurrently working with Cat...   \n2  SummaryI started my construction career in Jun...   \n3  SummaryCertified Electrical Foremanwith thirte...   \n4  SummaryWith extensive experience in business/r...   \n\n                                job_description_text   label  \n0  Net2Source Inc. is an award-winning total work...  No Fit  \n1  At Salas OBrien we tell our clients that were ...  No Fit  \n2  Schweitzer Engineering Laboratories (SEL) Infr...  No Fit  \n3  Mizick Miller & Company, Inc. is looking for a...  No Fit  \n4  Life at Capgemini\\nCapgemini supports all aspe...  No Fit  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>resume_text</th>\n      <th>job_description_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SummaryHighly motivated Sales Associate with e...</td>\n      <td>Net2Source Inc. is an award-winning total work...</td>\n      <td>No Fit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Professional SummaryCurrently working with Cat...</td>\n      <td>At Salas OBrien we tell our clients that were ...</td>\n      <td>No Fit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SummaryI started my construction career in Jun...</td>\n      <td>Schweitzer Engineering Laboratories (SEL) Infr...</td>\n      <td>No Fit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SummaryCertified Electrical Foremanwith thirte...</td>\n      <td>Mizick Miller &amp; Company, Inc. is looking for a...</td>\n      <td>No Fit</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SummaryWith extensive experience in business/r...</td>\n      <td>Life at Capgemini\\nCapgemini supports all aspe...</td>\n      <td>No Fit</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:57.956995Z","iopub.execute_input":"2025-01-13T13:31:57.957377Z","iopub.status.idle":"2025-01-13T13:31:57.968163Z","shell.execute_reply.started":"2025-01-13T13:31:57.957338Z","shell.execute_reply":"2025-01-13T13:31:57.966782Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(6241, 3)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:57.969296Z","iopub.execute_input":"2025-01-13T13:31:57.969728Z","iopub.status.idle":"2025-01-13T13:31:57.989440Z","shell.execute_reply.started":"2025-01-13T13:31:57.969676Z","shell.execute_reply":"2025-01-13T13:31:57.988426Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"resume_text             0\njob_description_text    0\nlabel                   0\ndtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:57.990483Z","iopub.execute_input":"2025-01-13T13:31:57.990865Z","iopub.status.idle":"2025-01-13T13:31:58.089094Z","shell.execute_reply.started":"2025-01-13T13:31:57.990828Z","shell.execute_reply":"2025-01-13T13:31:58.087965Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# remove duplicates\ndf= df.drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:58.090154Z","iopub.execute_input":"2025-01-13T13:31:58.090516Z","iopub.status.idle":"2025-01-13T13:31:58.177297Z","shell.execute_reply.started":"2025-01-13T13:31:58.090484Z","shell.execute_reply":"2025-01-13T13:31:58.176258Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:58.180417Z","iopub.execute_input":"2025-01-13T13:31:58.180752Z","iopub.status.idle":"2025-01-13T13:31:58.257929Z","shell.execute_reply.started":"2025-01-13T13:31:58.180717Z","shell.execute_reply":"2025-01-13T13:31:58.257058Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:58.259778Z","iopub.execute_input":"2025-01-13T13:31:58.260095Z","iopub.status.idle":"2025-01-13T13:31:58.270923Z","shell.execute_reply.started":"2025-01-13T13:31:58.260068Z","shell.execute_reply":"2025-01-13T13:31:58.269901Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"label\nNo Fit           3142\nPotential Fit    1556\nGood Fit         1542\nName: count, dtype: int64"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# number of category\ndf['label'].value_counts()\nprint(df['label'].value_counts())\n\n# check the maximum value counts\nprint('maximum',df['label'].value_counts().max())\nmax_size=df['label'].value_counts().max()\n# upsampling for balanced the classes\nbalanced_df=df.groupby('label').apply(lambda x: x.sample(max_size,replace=True)).reset_index(drop=True)\n\n# check the balance class\nprint('balanced class--',balanced_df['label'].value_counts())\n\n# shuffle the data to avoid any order\ndf=balanced_df.sample(frac=1).reset_index(drop=True)  # frac1 100% rows are shuffled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:58.271769Z","iopub.execute_input":"2025-01-13T13:31:58.272122Z","iopub.status.idle":"2025-01-13T13:31:58.305838Z","shell.execute_reply.started":"2025-01-13T13:31:58.272096Z","shell.execute_reply":"2025-01-13T13:31:58.305002Z"}},"outputs":[{"name":"stdout","text":"label\nNo Fit           3142\nPotential Fit    1556\nGood Fit         1542\nName: count, dtype: int64\nmaximum 3142\nbalanced class-- label\nGood Fit         3142\nNo Fit           3142\nPotential Fit    3142\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import re\nimport string\n\ndef clean_text(txt):\n    # Remove URLs\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    txt = pattern.sub(r'', txt)\n    \n    # Remove RT and cc\n    txt = re.sub(r'\\b(RT|cc)\\b', ' ', txt)\n    \n    # Remove Hashtags\n    txt = re.sub(r'#\\S+\\s', '', txt)\n    \n    # Remove Mentions\n    txt = re.sub(r'@\\S+', '', txt)\n    \n    # Remove Punctuation\n    exclude = string.punctuation\n    txt = txt.translate(str.maketrans('', '', exclude))\n    \n    # Remove Non-ASCII Characters\n    txt = re.sub(r'[^\\x00-\\x7f]', ' ', txt)\n    \n    # Normalize Whitespace\n    txt = re.sub('\\s+', ' ', txt).strip()\n    \n    return txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:58.307011Z","iopub.execute_input":"2025-01-13T13:31:58.307366Z","iopub.status.idle":"2025-01-13T13:31:58.313435Z","shell.execute_reply.started":"2025-01-13T13:31:58.307322Z","shell.execute_reply":"2025-01-13T13:31:58.312264Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df[['resume_text', 'job_description_text']] = df[['resume_text', 'job_description_text']].apply(lambda x: x.apply(clean_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:31:58.314444Z","iopub.execute_input":"2025-01-13T13:31:58.314682Z","iopub.status.idle":"2025-01-13T13:32:10.572368Z","shell.execute_reply.started":"2025-01-13T13:31:58.314660Z","shell.execute_reply":"2025-01-13T13:32:10.571350Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"res=df[['resume_text','job_description_text']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:10.573252Z","iopub.execute_input":"2025-01-13T13:32:10.573519Z","iopub.status.idle":"2025-01-13T13:32:10.579169Z","shell.execute_reply.started":"2025-01-13T13:32:10.573495Z","shell.execute_reply":"2025-01-13T13:32:10.578066Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\n# Combine resume and job description text into one list\nquestions = list(df['resume_text']) + list(df['job_description_text'])\n\n# Initialize TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=10000)\n\n# Fit and transform the combined text data\nquestions_tfidf = tfidf.fit_transform(questions).toarray()\n\n# Split the transformed array into two parts: one for resume_text and one for job_description_text\nq1_arr, q2_arr = np.vsplit(questions_tfidf, 2)\n\n# Verify shapes of the resulting arrays\nprint(\"Shape of q1_arr (resume_text):\", q1_arr.shape)\nprint(\"Shape of q2_arr (job_description_text):\", q2_arr.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:10.580263Z","iopub.execute_input":"2025-01-13T13:32:10.580617Z","iopub.status.idle":"2025-01-13T13:32:21.980061Z","shell.execute_reply.started":"2025-01-13T13:32:10.580580Z","shell.execute_reply":"2025-01-13T13:32:21.978840Z"}},"outputs":[{"name":"stdout","text":"Shape of q1_arr (resume_text): (9426, 10000)\nShape of q2_arr (job_description_text): (9426, 10000)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"temp_df1=pd.DataFrame(q1_arr,index=df.index)\ntemp_df2=pd.DataFrame(q2_arr,index=df.index)\ntemp_df=pd.concat([temp_df1,temp_df2],axis=1)\ntemp_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:21.980807Z","iopub.execute_input":"2025-01-13T13:32:21.981269Z","iopub.status.idle":"2025-01-13T13:32:25.654716Z","shell.execute_reply.started":"2025-01-13T13:32:21.981241Z","shell.execute_reply":"2025-01-13T13:32:25.653837Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(9426, 20000)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"temp_df['label']=df['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:25.655754Z","iopub.execute_input":"2025-01-13T13:32:25.656136Z","iopub.status.idle":"2025-01-13T13:32:25.672871Z","shell.execute_reply.started":"2025-01-13T13:32:25.656094Z","shell.execute_reply":"2025-01-13T13:32:25.671699Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb=LabelEncoder()\ntemp_df['label']=lb.fit_transform(temp_df['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:25.674026Z","iopub.execute_input":"2025-01-13T13:32:25.674417Z","iopub.status.idle":"2025-01-13T13:32:25.697517Z","shell.execute_reply.started":"2025-01-13T13:32:25.674380Z","shell.execute_reply":"2025-01-13T13:32:25.696603Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(temp_df.iloc[:,0:-1].values,temp_df.iloc[:,-1].values,test_size=0.2,random_state=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:25.698465Z","iopub.execute_input":"2025-01-13T13:32:25.698845Z","iopub.status.idle":"2025-01-13T13:32:29.470029Z","shell.execute_reply.started":"2025-01-13T13:32:25.698788Z","shell.execute_reply":"2025-01-13T13:32:29.469159Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"x_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:29.470977Z","iopub.execute_input":"2025-01-13T13:32:29.471287Z","iopub.status.idle":"2025-01-13T13:32:29.478022Z","shell.execute_reply.started":"2025-01-13T13:32:29.471260Z","shell.execute_reply":"2025-01-13T13:32:29.476960Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.06816864,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]])"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Ensure that X_train and X_test are dense if they are sparse\nx_train = x_train.toarray() if hasattr(x_train, 'toarray') else x_train\n\n# 1. Train RandomForestClassifier\nx_test = x_test.toarray() if hasattr(x_test, 'toarray') else x_test\n\nrf_model = OneVsRestClassifier(RandomForestClassifier())\nrf_model.fit(x_train, y_train)\ny_pred_rf = rf_model.predict(x_test)\nprint(\"\\nRandomForestClassifier Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_rf)}\")\nprint(f\"Classification Report:\\n{classification_report(y_test, y_pred_rf)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:32:29.479026Z","iopub.execute_input":"2025-01-13T13:32:29.479469Z","iopub.status.idle":"2025-01-13T13:33:45.311319Z","shell.execute_reply.started":"2025-01-13T13:32:29.479430Z","shell.execute_reply":"2025-01-13T13:33:45.310285Z"}},"outputs":[{"name":"stdout","text":"\nRandomForestClassifier Results:\nAccuracy: 0.9093\nConfusion Matrix:\n[[626   7   7]\n [ 60 505  55]\n [ 17  25 584]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.98      0.93       640\n           1       0.94      0.81      0.87       620\n           2       0.90      0.93      0.92       626\n\n    accuracy                           0.91      1886\n   macro avg       0.91      0.91      0.91      1886\nweighted avg       0.91      0.91      0.91      1886\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"xgb_model = OneVsRestClassifier(xgb.XGBClassifier())\nxgb_model.fit(x_train, y_train)\ny_pred_xgb = xgb_model.predict(x_test)\nprint(\"\\nXGBClassifier Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_xgb)}\")\nprint(f\"Classification Report:\\n{classification_report(y_test, y_pred_xgb)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:33:45.312319Z","iopub.execute_input":"2025-01-13T13:33:45.312600Z","iopub.status.idle":"2025-01-13T13:36:15.613145Z","shell.execute_reply.started":"2025-01-13T13:33:45.312576Z","shell.execute_reply":"2025-01-13T13:36:15.612105Z"}},"outputs":[{"name":"stdout","text":"\nXGBClassifier Results:\nAccuracy: 0.9136\nConfusion Matrix:\n[[619  15   6]\n [ 51 519  50]\n [ 19  22 585]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.97      0.93       640\n           1       0.93      0.84      0.88       620\n           2       0.91      0.93      0.92       626\n\n    accuracy                           0.91      1886\n   macro avg       0.91      0.91      0.91      1886\nweighted avg       0.91      0.91      0.91      1886\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def pred(input_resume, input_job_description):\n    # Preprocess both the resume and job description\n    cleaned_resume = clean_text(input_resume)\n    cleaned_job_desc = clean_text(input_job_description)\n    \n    # Vectorize the cleaned texts using the TF-IDF vectorizer\n    # We assume `q1_arr` corresponds to the resume features and `q2_arr` corresponds to job description features\n    vectorized_resume = tfidf.transform([cleaned_resume]).toarray()\n    vectorized_job_desc = tfidf.transform([cleaned_job_desc]).toarray()\n    \n    # Concatenate the vectors from both the resume and job description\n    combined_vector = np.concatenate((vectorized_resume, vectorized_job_desc), axis=1)\n    \n    # Make prediction using the model (xgb_model)\n    predicted_label = xgb_model.predict(combined_vector)\n    # print(\"pr\"+predicted_label)\n    \n    # Convert the predicted label to its corresponding category name\n    predicted_label_name = lb.inverse_transform(predicted_label)\n    \n    return predicted_label_name[0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:36:15.614200Z","iopub.execute_input":"2025-01-13T13:36:15.614556Z","iopub.status.idle":"2025-01-13T13:36:15.620230Z","shell.execute_reply.started":"2025-01-13T13:36:15.614526Z","shell.execute_reply":"2025-01-13T13:36:15.618893Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"resume = \"\"\"\n\nHere's an example of a resume that could align well with the job description you provided for the Data Scientist Intern role. This resume emphasizes the relevant skills and experiences while keeping the language focused on the job's key requirements:\n\nSAHIL YADAV\nChhpara Salimpur, District-Mohindergarh, Haryana\nEmail: sahilchhapra01@gmail.com\nLinkedIn: linkedin.com/in/sahilyadav\nGitHub: github.com/sahilyadav\n\nObjective\nA passionate and driven Computer Science Engineering student with a strong foundation in Data Science, Machine Learning, and Python programming. Seeking to leverage my skills in data analysis, machine learning, and statistical modeling in a Data Scientist Intern position at OptimSpace to gain hands-on experience and contribute to data-driven solutions.\n\nEducation\nB.Tech in Computer Science Engineering (CSE)\nJECRCE University\nExpected Graduation: 2026\nCGPA: 9.35 (First Year)\n\nKey Skills\nProgramming Languages: Python, Java, SQL\nData Analysis & Visualization: Pandas, NumPy, Matplotlib, Seaborn\nMachine Learning: Scikit-learn, XGBoost, Keras, TensorFlow\nStatistical Modeling: Regression, Classification, Clustering\nData Preprocessing: Data cleaning, feature extraction, handling missing data\nDatabase Management: SQL, MySQL\nTools & Platforms: Jupyter, Google Cloud, Databricks, Streamlit\nSoft Skills: Critical thinking, problem-solving, teamwork, communication\nProjects\nStock Price Prediction (LSTM Model)\nDeveloped a time-series prediction model using LSTM to forecast stock prices based on historical data.\nImplemented data preprocessing techniques for handling missing values, scaling, and feature extraction.\nCredit Risk Modeling\nBuilt a robust credit risk model using XGBoost to predict loan defaults, providing insights for better financial decision-making.\nApplied data preprocessing, feature engineering, and model evaluation techniques for improved performance.\nResume Screening Platform (AI-based)\nDesigned an NLP-based resume screening system that automates candidate selection by analyzing resumes and matching them with job descriptions.\nImplemented text classification, keyword extraction, and semantic analysis using spaCy and NLTK.\nChest X-Ray Classification (Deep Learning)\nCreated a deep learning model for classifying chest X-rays into categories like viral pneumonia, bacterial pneumonia, and tuberculosis using CNNs.\nDeployed the model using TensorFlow and Python.\nPersonality Analysis Model\nDeveloped a personality analysis model to assess and understand traits across different departments to optimize team dynamics and recruitment processes.\nDeployed the model on Streamlit for easy access by the HR team.\nInternship Experience\nMS FINCAP (NBFC)\nOctober 2023 - Ongoing\n\nDeveloped AI-driven tools for detecting fake videos and voice cloning, improving security and preventing fraud.\nCollaborated with data scientists to analyze financial data and build predictive models for loan approval.\nCertifications\nGoogle Cloud Certified: Google Cloud Foundation\nDatabricks Fundamentals Certificate\nParticipated in Hackathons: Data Science & Innovation (NPTEL)\nExtracurricular Activities\nHackathon Winner: Developed a Machine Learning Model for stock price prediction.\nVolunteer: Assisted in organizing tech workshops and seminars for beginners in data science and programming.\nLanguages\nEnglish: Fluent\nHindi: Fluent\nHobbies\nReading: Exploring new technologies and advancements in AI/ML.\nCoding: Passionate about learning new programming languages and building projects.\nGaming: Engaging in strategy games to enhance problem-solving skills\n\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:36:15.623961Z","iopub.execute_input":"2025-01-13T13:36:15.624295Z","iopub.status.idle":"2025-01-13T13:36:15.641473Z","shell.execute_reply.started":"2025-01-13T13:36:15.624265Z","shell.execute_reply":"2025-01-13T13:36:15.640284Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"job_description = \"\"\"\n\nSkills:\nData Analysis, Statistical Modeling, Machine Learning, Python Programming, Data Visualization, SQL, Critical Thinking, Problem Solving,\n\nJob Overview\n\nOptimSpace is seeking a highly motivated Data Scientist Intern to join our team. This is a remote internship opportunity available to freshers with a passion for data science. Locations include Noida, Hyderabad, Chennai, Gurgaon, Pune, Bangalore, and Mumbai. The position is a perfect fit for individuals ready to craft their skills in data analysis and enhance their career pathway in the technology industry.\n\nQualifications And Skills\n\nProficiency in data analysis to interpret and derive meaningful insights from complex data sets (Mandatory skill).\nStrong Python programming skills for data manipulation, analysis, and model building (Mandatory skill).\nKnowledge of SQL for efficient querying and managing of databases (Mandatory skill).\nExperience in statistical modeling to help understand trends and make predictions from data.\nUnderstanding of machine learning concepts and algorithms to drive business solutions.\nAbility to create data visualizations to effectively communicate findings and insights.\nCritical thinking capability to analyze facts for informed decision-making processes.\nExcellent problem solving skills to tackle challenges through analytical approaches.\n\nRoles And Responsibilities\n\nCollaborate with the data team to perform data cleaning, preprocessing, and analysis tasks.\nAssist in building predictive models and algorithms to support business objectives.\nDevelop and implement data visualization tools to provide insights to stakeholders.\nSupport the development of data-driven strategies by formulating novel data analysis techniques.\nContribute to improving data collection procedures to include information relevant for building analytic systems.\nWork under the guidance of senior data scientists to gain practical knowledge and experience.\nDocument processes and findings to contribute to a cohesive data knowledge base.\nParticipate in training sessions and feedback loops to enhance data competency skills.\nDesired Skills and Experience\nData Analysis, Statistical Modeling, Machine Learning, Python Programming, Data Visualization, SQL, Critical Thinking, Problem Solving\n\n\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:36:15.642947Z","iopub.execute_input":"2025-01-13T13:36:15.643399Z","iopub.status.idle":"2025-01-13T13:36:15.663077Z","shell.execute_reply.started":"2025-01-13T13:36:15.643364Z","shell.execute_reply":"2025-01-13T13:36:15.661932Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"pred(resume,job_description)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:36:41.746069Z","iopub.execute_input":"2025-01-13T13:36:41.746387Z","iopub.status.idle":"2025-01-13T13:36:41.761932Z","shell.execute_reply.started":"2025-01-13T13:36:41.746358Z","shell.execute_reply":"2025-01-13T13:36:41.760822Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'No Fit'"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"print(lb.classes_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:36:15.692022Z","iopub.execute_input":"2025-01-13T13:36:15.692280Z","iopub.status.idle":"2025-01-13T13:36:15.705131Z","shell.execute_reply.started":"2025-01-13T13:36:15.692256Z","shell.execute_reply":"2025-01-13T13:36:15.704246Z"}},"outputs":[{"name":"stdout","text":"['Good Fit' 'No Fit' 'Potential Fit']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import pickle\npickle.dump(tfidf,open('tfidf1.pkl','wb'))\npickle.dump(xgb_model, open('xgb.pkl', 'wb'))\npickle.dump(lb, open(\"encoder1.pkl\",'wb'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T13:37:24.340315Z","iopub.execute_input":"2025-01-13T13:37:24.340665Z","iopub.status.idle":"2025-01-13T13:37:24.393605Z","shell.execute_reply.started":"2025-01-13T13:37:24.340636Z","shell.execute_reply":"2025-01-13T13:37:24.392798Z"}},"outputs":[],"execution_count":28}]}